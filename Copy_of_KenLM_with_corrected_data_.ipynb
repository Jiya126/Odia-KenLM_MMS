{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "-14abksUyNRA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68dcabc7-713e-4b03-b201-16a3df971df4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "libboost-program-options-dev is already the newest version (1.74.0.3ubuntu7).\n",
            "libboost-program-options-dev set to manually installed.\n",
            "libboost-system-dev is already the newest version (1.74.0.3ubuntu7).\n",
            "libboost-system-dev set to manually installed.\n",
            "libboost-thread-dev is already the newest version (1.74.0.3ubuntu7).\n",
            "libboost-thread-dev set to manually installed.\n",
            "libbz2-dev is already the newest version (1.0.8-5build1).\n",
            "libbz2-dev set to manually installed.\n",
            "liblzma-dev is already the newest version (5.2.5-2ubuntu1).\n",
            "liblzma-dev set to manually installed.\n",
            "libboost-test-dev is already the newest version (1.74.0.3ubuntu7).\n",
            "libboost-test-dev set to manually installed.\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.1).\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-2ubuntu9.2).\n",
            "zlib1g-dev set to manually installed.\n",
            "Suggested packages:\n",
            "  libeigen3-doc libmpfrc++-dev\n",
            "The following NEW packages will be installed:\n",
            "  libeigen3-dev\n",
            "0 upgraded, 1 newly installed, 0 to remove and 16 not upgraded.\n",
            "Need to get 1,056 kB of archives.\n",
            "After this operation, 9,081 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libeigen3-dev all 3.4.0-2ubuntu2 [1,056 kB]\n",
            "Fetched 1,056 kB in 0s (9,055 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libeigen3-dev.\n",
            "(Reading database ... 120831 files and directories currently installed.)\n",
            "Preparing to unpack .../libeigen3-dev_3.4.0-2ubuntu2_all.deb ...\n",
            "Unpacking libeigen3-dev (3.4.0-2ubuntu2) ...\n",
            "Setting up libeigen3-dev (3.4.0-2ubuntu2) ...\n",
            "--2023-08-22 17:39:53--  https://kheafield.com/code/kenlm.tar.gz\n",
            "Resolving kheafield.com (kheafield.com)... 35.196.63.85\n",
            "Connecting to kheafield.com (kheafield.com)|35.196.63.85|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 491888 (480K) [application/x-gzip]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>] 480.36K  1012KB/s    in 0.5s    \n",
            "\n",
            "2023-08-22 17:39:54 (1012 KB/s) - written to stdout [491888/491888]\n",
            "\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
            "  Compatibility with CMake < 3.5 will be removed from a future version of\n",
            "  CMake.\n",
            "\n",
            "  Update the VERSION argument <min> value or use a ...<max> suffix to tell\n",
            "  CMake that the project does not need compatibility with older versions.\n",
            "\n",
            "\u001b[0m\n",
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found Boost: /usr/lib/x86_64-linux-gnu/cmake/Boost-1.74.0/BoostConfig.cmake (found suitable version \"1.74.0\", minimum required is \"1.41.0\") found components: program_options system thread unit_test_framework \n",
            "-- Found Threads: TRUE  \n",
            "-- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version \"1.2.11\")  \n",
            "-- Found BZip2: /usr/lib/x86_64-linux-gnu/libbz2.so (found version \"1.0.8\") \n",
            "-- Looking for BZ2_bzCompressInit\n",
            "-- Looking for BZ2_bzCompressInit - found\n",
            "-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
            "-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
            "-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
            "-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
            "-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
            "-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
            "-- Found LibLZMA: /usr/lib/x86_64-linux-gnu/liblzma.so (found version \"5.2.5\") \n",
            "-- Looking for clock_gettime in rt\n",
            "-- Looking for clock_gettime in rt - found\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\") \n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \n",
            "-- Found OpenMP: TRUE (found version \"4.5\")  \n",
            "-- Configuring done (1.6s)\n",
            "-- Generating done (0.1s)\n",
            "-- Build files have been written to: /content/kenlm/build\n",
            "[  1%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum-dtoa.cc.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum.cc.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/cached-powers.cc.o\u001b[0m\n",
            "[  4%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/diy-fp.cc.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/double-conversion.cc.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fast-dtoa.cc.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fixed-dtoa.cc.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/strtod.cc.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/chain.cc.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/count_records.cc.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/io.cc.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/line_input.cc.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/multi_progress.cc.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/rewindable_stream.cc.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/bit_packing.cc.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/ersatz_progress.cc.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/exception.cc.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file.cc.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file_piece.cc.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/float_to_string.cc.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/integer_to_string.cc.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/mmap.cc.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/murmur_hash.cc.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/parallel_read.cc.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/pool.cc.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/read_compressed.cc.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/scoped.cc.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/spaces.cc.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/string_piece.cc.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/usage.cc.o\u001b[0m\n",
            "[ 32%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm_util.a\u001b[0m\n",
            "[ 32%] Built target kenlm_util\n",
            "[ 33%] \u001b[32mBuilding CXX object util/CMakeFiles/probing_hash_table_benchmark.dir/probing_hash_table_benchmark_main.cc.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/bhiksha.cc.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/binary_format.cc.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/config.cc.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/lm_exception.cc.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/model.cc.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/quantize.cc.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/read_arpa.cc.o\u001b[0m\n",
            "[ 41%] \u001b[32m\u001b[1mLinking CXX executable ../bin/probing_hash_table_benchmark\u001b[0m\n",
            "[ 41%] Built target probing_hash_table_benchmark\n",
            "[ 42%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/arpa_io.cc.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_hashed.cc.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/phrase.cc.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_trie.cc.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/vocab.cc.o\u001b[0m\n",
            "[ 47%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_filter.a\u001b[0m\n",
            "[ 47%] Built target kenlm_filter\n",
            "[ 48%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/sizes.cc.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie.cc.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie_sort.cc.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/value_build.cc.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/virtual_interface.cc.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/vocab.cc.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/model_buffer.cc.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/print.cc.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/renumber.cc.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/size_option.cc.o\u001b[0m\n",
            "[ 59%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm.a\u001b[0m\n",
            "[ 59%] Built target kenlm\n",
            "[ 61%] \u001b[32mBuilding CXX object lm/CMakeFiles/fragment.dir/fragment_main.cc.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object lm/CMakeFiles/query.dir/query_main.cc.o\u001b[0m\n",
            "[ 62%] \u001b[32m\u001b[1mLinking CXX executable ../bin/fragment\u001b[0m\n",
            "[ 62%] Built target fragment\n",
            "[ 63%] \u001b[32mBuilding CXX object lm/CMakeFiles/build_binary.dir/build_binary_main.cc.o\u001b[0m\n",
            "[ 64%] \u001b[32m\u001b[1mLinking CXX executable ../bin/query\u001b[0m\n",
            "[ 64%] Built target query\n",
            "[ 65%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm_benchmark.dir/kenlm_benchmark_main.cc.o\u001b[0m\n",
            "[ 66%] \u001b[32m\u001b[1mLinking CXX executable ../bin/build_binary\u001b[0m\n",
            "[ 66%] Built target build_binary\n",
            "[ 67%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/adjust_counts.cc.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/corpus_count.cc.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/initial_probabilities.cc.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/interpolate.cc.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../bin/kenlm_benchmark\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/output.cc.o\u001b[0m\n",
            "[ 72%] Built target kenlm_benchmark\n",
            "[ 73%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/filter.dir/filter_main.cc.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/pipeline.cc.o\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_builder.a\u001b[0m\n",
            "[ 76%] Built target kenlm_builder\n",
            "[ 77%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/phrase_table_vocab.dir/phrase_table_vocab_main.cc.o\u001b[0m\n",
            "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/filter\u001b[0m\n",
            "[ 78%] Built target filter\n",
            "[ 79%] \u001b[32mBuilding CXX object lm/interpolate/CMakeFiles/kenlm_interpolate.dir/backoff_reunification.cc.o\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/phrase_table_vocab\u001b[0m\n",
            "[ 80%] Built target phrase_table_vocab\n",
            "[ 81%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/lmplz.dir/lmplz_main.cc.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object lm/interpolate/CMakeFiles/kenlm_interpolate.dir/bounded_sequence_encoding.cc.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object lm/interpolate/CMakeFiles/kenlm_interpolate.dir/merge_probabilities.cc.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object lm/interpolate/CMakeFiles/kenlm_interpolate.dir/merge_vocab.cc.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object lm/interpolate/CMakeFiles/kenlm_interpolate.dir/normalize.cc.o\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/lmplz\u001b[0m\n",
            "[ 86%] Built target lmplz\n",
            "[ 87%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/count_ngrams.dir/count_ngrams_main.cc.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object lm/interpolate/CMakeFiles/kenlm_interpolate.dir/pipeline.cc.o\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/count_ngrams\u001b[0m\n",
            "[ 89%] Built target count_ngrams\n",
            "[ 90%] \u001b[32mBuilding CXX object lm/interpolate/CMakeFiles/kenlm_interpolate.dir/split_worker.cc.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object lm/interpolate/CMakeFiles/kenlm_interpolate.dir/tune_derivatives.cc.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object lm/interpolate/CMakeFiles/kenlm_interpolate.dir/tune_instances.cc.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object lm/interpolate/CMakeFiles/kenlm_interpolate.dir/tune_weights.cc.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object lm/interpolate/CMakeFiles/kenlm_interpolate.dir/universal_vocab.cc.o\u001b[0m\n",
            "[ 95%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_interpolate.a\u001b[0m\n",
            "[ 95%] Built target kenlm_interpolate\n",
            "[ 97%] \u001b[32mBuilding CXX object lm/interpolate/CMakeFiles/streaming_example.dir/streaming_example_main.cc.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object lm/interpolate/CMakeFiles/interpolate.dir/interpolate_main.cc.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/interpolate\u001b[0m\n",
            "[ 98%] Built target interpolate\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/streaming_example\u001b[0m\n",
            "[100%] Built target streaming_example\n",
            "build_binary  fragment\t       lmplz\t\t\t     query\n",
            "count_ngrams  interpolate      phrase_table_vocab\t     streaming_example\n",
            "filter\t      kenlm_benchmark  probing_hash_table_benchmark\n"
          ]
        }
      ],
      "source": [
        "# installing kenlm\n",
        "!sudo apt install build-essential cmake libboost-system-dev libboost-thread-dev libboost-program-options-dev libboost-test-dev libeigen3-dev zlib1g-dev libbz2-dev liblzma-dev\n",
        "!wget -O - https://kheafield.com/code/kenlm.tar.gz | tar xz\n",
        "!mkdir kenlm/build && cd kenlm/build && cmake .. && make -j2\n",
        "!ls kenlm/build/bin"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# getting dataset for kenlm training\n",
        "file_id ='1CyXW_wNWx9_Kjo8v8hNjFdnTBq2qmh5f'\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from google.colab import drive\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "download = drive.CreateFile({'id': file_id})\n",
        "# Download the file to a local disc\n",
        "download.GetContentFile('odia.zip')"
      ],
      "metadata": {
        "id": "gHGXQBTwy1DH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing characters not in dict from training data"
      ],
      "metadata": {
        "id": "63y7VmZ8Hh_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('mms_dict_odia.txt', 'r', encoding=\"utf8\") as dictf:\n",
        "    dict_list = []\n",
        "    for d in dictf:\n",
        "        # print(d.strip())\n",
        "        char = d.split()[0]\n",
        "        # print(char)\n",
        "        dict_list.append(char)\n",
        "    print(dict_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yb5Wiqq_Hxq4",
        "outputId": "67433faf-0d81-41cb-f68d-eecc40aad00f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['|', '୍', 'ା', 'ର', 'ି', 'କ', 'େ', 'ବ', 'ନ', 'ତ', 'ସ', 'ପ', 'ୁ', 'ମ', 'ହ', 'ୟ', 'ଲ', 'ଏ', 'ଥ', 'ଦ', 'ୋ', 'ଣ', 'ଇ', 'ଟ', 'ଗ', 'ଯ', 'ଅ', 'ୀ', 'ଜ', 'ଶ', 'ଆ', 'ଷ', 'ଡ', 'ଧ', 'ଭ', 'ଳ', 'ଙ', 'ଚ', 'ଉ', 'ଂ', 'ଛ', 'ଁ', 'ଖ', 'ୂ', 'ଫ', 'ୱ', '଼', 'ଠ', 'ୃ', 'ଓ', '0', '1', '।', 'ଘ', 'ଞ', 'ୌ', '\\u200c', '2', 'ଵ', '-', '9', 'ୈ', '5', 'ଃ', '4', '6', '3', '8', 'ଢ', '.', 'a', 'ଝ', '7', 'm', 'n', 'c', 'i', 'e', \"'\", 'p', 't', '/', 's', 'o', 'd', ',', 'r', ':', 'b', '’', 'l', 'h', 'u', '\\u200d', 'ଋ', 'w', 'ଔ', 'ଐ', 'k', 'g', '%', 'ଈ', 'f', '\"', 'v', 'z', 'y', 'x', '[', ']', ';', '¥', '‘', '”', '୦', '+', 'õ', '*', 'ୗ', 'j', 'ୄ', '°', '୭', '£', '!', 'q', '$']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('odia/lm_train.txt', 'r', encoding=\"utf8\") as dictf:\n",
        "    char_list = []\n",
        "    for d in dictf:\n",
        "        # print(d.strip())\n",
        "        word_sent = []\n",
        "        for l in d:\n",
        "            word = l.split()\n",
        "            # print(word)\n",
        "            word_sent.append(word)\n",
        "        char_list.append(word_sent)"
      ],
      "metadata": {
        "id": "bGFFC_AuH-nE"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(char_list))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYkHY6UhQSn4",
        "outputId": "e8a68141-950e-41c6-c3ac-947220dac45a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "59781\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(char_list[0])\n",
        "\n",
        "for sent_char_list in char_list:\n",
        "  for ind_char in sent_char_list:\n",
        "    if ind_char == []:\n",
        "      pass\n",
        "    elif ind_char[0] in dict_list:\n",
        "      # print(ind_char)\n",
        "      pass\n",
        "    else:\n",
        "      print('not')\n",
        "      # print(ind_char)\n",
        "      # print('/////')\n",
        "      char_list.remove(sent_char_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewiKEpyxISpp",
        "outputId": "3000882b-77cd-4eb6-e41d-05d714117577"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['ସ'], ['ା'], ['ର'], [], ['ହ'], ['ା'], ['ତ'], [], ['ଗ'], ['ୋ'], ['ଡ'], ['଼'], [], ['କ'], ['ା'], ['ଲ'], ['ୁ'], ['ଆ'], [], ['ମ'], ['ା'], ['ର'], ['ି'], [], ['ଯ'], ['ା'], ['ଉ'], ['ଛ'], ['ି'], [], ['ଦ'], ['େ'], ['ହ'], ['ର'], [], ['ଗ'], ['ତ'], ['ର'], [], ['ନ'], ['ା'], ['ହ'], ['ି'], ['ଁ'], []]\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n",
            "not\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(char_list))\n",
        "print(char_list[0:4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6PK6vpKQQWd",
        "outputId": "99f38817-7594-4635-ba31-e52903f27563"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "59638\n",
            "[[['ସ'], ['ା'], ['ର'], [], ['ହ'], ['ା'], ['ତ'], [], ['ଗ'], ['ୋ'], ['ଡ'], ['଼'], [], ['କ'], ['ା'], ['ଲ'], ['ୁ'], ['ଆ'], [], ['ମ'], ['ା'], ['ର'], ['ି'], [], ['ଯ'], ['ା'], ['ଉ'], ['ଛ'], ['ି'], [], ['ଦ'], ['େ'], ['ହ'], ['ର'], [], ['ଗ'], ['ତ'], ['ର'], [], ['ନ'], ['ା'], ['ହ'], ['ି'], ['ଁ'], []], [['କ'], ['େ'], ['ଉ'], ['ଁ'], [], ['ସ'], ['ମ'], ['ୟ'], ['ର'], ['େ'], [], ['ମ'], ['ୁ'], ['ଗ'], [], ['ବ'], ['ି'], ['କ'], ['୍'], ['ର'], ['ି'], [], ['କ'], ['ଲ'], ['େ'], [], ['ମ'], ['ୁ'], ['ଁ'], [], ['ଅ'], ['ଧ'], ['ି'], ['କ'], [], ['ଦ'], ['ା'], ['ମ'], ['୍'], [], ['ପ'], ['ା'], ['ଇ'], [], ['ପ'], ['ା'], ['ର'], ['ି'], ['ବ'], ['ି'], []], [['ଶ'], ['ସ'], ['୍'], ['ୟ'], ['କ'], ['ୁ'], [], ['ନ'], ['ି'], ['ଜ'], [], ['ଅ'], ['ଞ'], ['୍'], ['ଚ'], ['ଳ'], [], ['ଛ'], ['ଡ'], ['ା'], [], ['ବ'], ['ା'], ['ହ'], ['ା'], ['ର'], ['େ'], [], ['ବ'], ['ି'], ['କ'], ['୍'], ['ର'], ['ି'], [], ['କ'], ['ର'], ['ି'], ['ବ'], ['ା'], [], ['ପ'], ['ା'], ['ଇ'], ['ଁ'], [], ['ସ'], ['ର'], ['କ'], ['ା'], ['ର'], ['ୀ'], [], ['ସ'], ['୍'], ['ତ'], ['ର'], ['ର'], ['େ'], [], ['କ'], ['ି'], ['ଛ'], ['ି'], [], ['ସ'], ['ୁ'], ['ବ'], ['ି'], ['ଧ'], ['ା'], [], ['ଅ'], ['ଛ'], ['ି'], [], ['କ'], ['ି'], []], [['ସ'], ['ବ'], ['ୁ'], ['ଆ'], ['ଡ'], ['େ'], [], ['ଦ'], ['େ'], ['ଖ'], ['େ'], ['ଇ'], ['ଲ'], ['ୁ'], [], ['ଭ'], ['ଲ'], [], ['ହ'], ['େ'], ['ଲ'], ['ା'], ['ନ'], ['ି'], [], ['ଏ'], ['ଇ'], [], ['ଡ'], ['଼'], ['ା'], ['କ'], ['୍'], ['ତ'], ['ର'], [], ['ପ'], ['ା'], ['ର'], ['ି'], ['ବ'], ['େ'], [], ['କ'], ['ି'], []]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm --r 'new_lm_train.txt'"
      ],
      "metadata": {
        "id": "3glY-7vkTPOD"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data_path = '/content/new_lm_train.txt'\n",
        "for inner_list in char_list:\n",
        "  conc_str = [element[0] if element else ' ' for element in inner_list]\n",
        "  new_data = ''.join(conc_str) + '\\n'\n",
        "  with open(new_data_path, 'a') as file:\n",
        "    file.write(new_data)"
      ],
      "metadata": {
        "id": "md7wDgDnQzNR"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making an KenLM dictionary"
      ],
      "metadata": {
        "id": "SPhxNtYHg2cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip odia.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDaZQKId7fXI",
        "outputId": "7ea66906-98e7-40e9-b8bc-590bf91a403f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  odia.zip\n",
            "   creating: odia/\n",
            "  inflating: odia/lm_proper_nouns_ai4bharat_train.txt  \n",
            "  inflating: odia/lm_train.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#building arpa file from training data (kenLM)\n",
        "!kenlm/build/bin/lmplz -o 5 <\"new_lm_train.txt\" > \"5gram.arpa\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHF7SgtOWx7Q",
        "outputId": "ed5d5c8f-a352-4e9f-f525-546e6646912d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1/5 Counting and sorting n-grams ===\n",
            "Reading /content/new_lm_train.txt\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Unigram tokens 541214 types 1571\n",
            "=== 2/5 Calculating and sorting adjusted counts ===\n",
            "Chain sizes: 1:18852 2:1062500672 3:1992188800 4:3187502080 5:4648440832\n",
            "/content/kenlm/lm/builder/adjust_counts.cc:49 in void lm::builder::{anonymous}::StatCollector::CalculateDiscounts(const lm::builder::DiscountConfig&) threw BadDiscountException because `s.n[j] == 0'.\n",
            "Could not calculate Kneser-Ney discounts for 5-grams with adjusted count 2 because we didn't observe any 5-grams with adjusted count 1; Is this small or artificial data?\n",
            "Try deduplicating the input.  To override this error for e.g. a class-based model, rerun with --discount_fallback\n",
            "\n",
            "/bin/bash: line 1: 17299 Aborted                 (core dumped) kenlm/build/bin/lmplz -o 5 < \"new_lm_train.txt\" > \"5gram.arpa\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# correcting arpa file generated from kenlm function\n",
        "with open(\"5gram.arpa\", \"r\") as read_file, open(\"5gram_correct.arpa\", \"w\") as write_file:\n",
        "  has_added_eos = False\n",
        "  for line in read_file:\n",
        "    if not has_added_eos and \"ngram 1=\" in line:\n",
        "      count=line.strip().split(\"=\")[-1]\n",
        "      write_file.write(line.replace(f\"{count}\", f\"{int(count)+1}\"))\n",
        "    elif not has_added_eos and \"<s>\" in line:\n",
        "      write_file.write(line)\n",
        "      write_file.write(line.replace(\"<s>\", \"</s>\"))\n",
        "      has_added_eos = True\n",
        "    else:\n",
        "      write_file.write(line)\n"
      ],
      "metadata": {
        "id": "Hkk6rH8vXCUQ"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/bin/bash\n",
        "\n",
        "# Assign the filename to a variable\n",
        "input_file=\"new_lm_train.txt\"\n",
        "\n",
        "\n",
        "\n",
        "def create_models_files (input_file,output_file,lexicon_file ):\n",
        "\n",
        "### Making the arpa files\n",
        "\n",
        "  output_file1 =  output_file + \".arpa\"\n",
        "  output_file2 =  output_file + \"_correct.arpa\"\n",
        "  output_bin_file =  output_file + \".bin\"\n",
        "\n",
        "  !kenlm/build/bin/lmplz -o 5 <\"$input_file\" > \"$output_file1\"  --discount_fallback\n",
        "\n",
        "### adding the </s> character to the arpa file\n",
        "\n",
        "  with open(output_file1, \"r\") as read_file, open(output_file2, \"w\") as write_file:\n",
        "    has_added_eos = False\n",
        "    for line in read_file:\n",
        "      if not has_added_eos and \"ngram 1=\" in line:\n",
        "        count=line.strip().split(\"=\")[-1]\n",
        "        write_file.write(line.replace(f\"{count}\", f\"{int(count)+1}\"))\n",
        "      elif not has_added_eos and \"<s>\" in line:\n",
        "        write_file.write(line)\n",
        "        write_file.write(line.replace(\"<s>\", \"</s>\"))\n",
        "        has_added_eos = True\n",
        "      else:\n",
        "        write_file.write(line)\n",
        "\n",
        "### converting arpa file to bin file\n",
        "\n",
        "  !kenlm/build/bin/build_binary \"$output_file2\" \"$output_bin_file\"\n",
        "  print (\"The file is created at :\",\"output_bin_file\")\n",
        "\n",
        "### creating the lexicon file\n",
        "\n",
        "  with open(input_file, 'r', encoding='utf8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Tokenize the text into words\n",
        "  words = text.split()\n",
        "\n",
        "# Extract unique words\n",
        "  unique_words = set(words)\n",
        "\n",
        "  with open(lexicon_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for word in unique_words:\n",
        "        # Split the word into its characters\n",
        "        phonemes = \" \".join(list(word)) + \" |\"\n",
        "        # Write the word and its phonemes to the lexicon file\n",
        "        # f.write(f\"{word}\\t{phonemes}\\n\")\n",
        "        # f.write(f\"{phonemes}\\n\")\n",
        "        f.write(f\"{word}\\t{phonemes}\\n\")\n",
        "\n",
        "  return( )"
      ],
      "metadata": {
        "id": "lQo-D68QdmZa"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r output_results_new"
      ],
      "metadata": {
        "id": "y3ggIeaYmn_9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbe42efd-0923-4898-b1d7-eb67d5488925"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'output_results_new': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating bin and lexicon file in 'output_results' folder\n",
        "input_file=\"new_lm_train.txt\"\n",
        "\n",
        "!mkdir output_results_new\n",
        "output_file =  \"output_results_new/new_5gram_test\"\n",
        "lexicon_file =  \"output_results_new/new_lexicon_test.txt\"\n",
        "\n",
        "create_models_files (input_file,output_file,lexicon_file )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slO9KJgBkHNw",
        "outputId": "f1376b4e-5ccc-4ae7-bb1b-bab26b0cfc4a"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1/5 Counting and sorting n-grams ===\n",
            "Reading /content/new_lm_train.txt\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Unigram tokens 541214 types 1571\n",
            "=== 2/5 Calculating and sorting adjusted counts ===\n",
            "Chain sizes: 1:18852 2:1062500672 3:1992188800 4:3187502080 5:4648440832\n",
            "Substituting fallback discounts for order 4: D1=0.5 D2=1 D3+=1.5\n",
            "Statistics:\n",
            "1 1571 D1=0.621429 D2=1.09248 D3+=1.65116\n",
            "2 5061 D1=0.823319 D2=1.249 D3+=1.48753\n",
            "3 6344 D1=0.913014 D2=1.50487 D3+=1.29053\n",
            "4 6288 D1=0.962104 D2=1.81118 D3+=1.35068\n",
            "5 5692 D1=0.5 D2=1 D3+=1.5\n",
            "Memory estimate for binary LM:\n",
            "type     kB\n",
            "probing 554 assuming -p 1.5\n",
            "probing 664 assuming -r models -p 1.5\n",
            "trie    254 without quantization\n",
            "trie    143 assuming -q 8 -b 8 quantization \n",
            "trie    241 assuming -a 22 array pointer compression\n",
            "trie    130 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
            "=== 3/5 Calculating and sorting initial probabilities ===\n",
            "Chain sizes: 1:18852 2:80976 3:126880 4:150912 5:159376\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
            "Chain sizes: 1:18852 2:80976 3:126880 4:150912 5:159376\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 5/5 Writing ARPA model ===\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Name:lmplz\tVmPeak:10807256 kB\tVmRSS:6708 kB\tRSSMax:1883788 kB\tuser:0.289765\tsys:1.01815\tCPU:1.30796\treal:1.35639\n",
            "Reading output_results_new/new_5gram_test_correct.arpa\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "SUCCESS\n",
            "The file is created at : output_bin_file\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "()"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    }
  ]
}